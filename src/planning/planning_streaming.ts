import { traceable } from "langsmith/traceable";
import { SystemMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { advancedModel, balancedModel, fastModel } from "../config";
import { storeChatMessage, retrieverSessionHistory } from "../memory";
import { specificQueryTool, aggregateTool, hybridSearchTool, calculatorTool } from "../tools";


// ===== OpenAiChatCompleiton COM STREAMING (CORRIGIDO) =====
export const OpenAiChatCompleiton = traceable(
    async function OpenAiChatCompleiton(
        messages: any,
        modelType: "advanced" | "balanced" | "fast" = "advanced",
        onChunk?: (chunk: string) => void  // ‚Üê Callback para streaming (opcional)
    ): Promise<string> {
        try {
            const langchainMessages = messages.map((msg: any) => {
                if (msg.role === "system") return new SystemMessage(msg.content);
                if (msg.role === "user") return new HumanMessage(msg.content);
                if (msg.role === "assistant") return new AIMessage(msg.content);
                return new HumanMessage(msg.content); 
            });

            // Seleciona o modelo
            let selectedModel;
            if (modelType === "advanced") {
                selectedModel = advancedModel;
            } else if (modelType === "balanced") {
                selectedModel = balancedModel;
            } else {
                selectedModel = fastModel;
            }

            // ‚úÖ SE TEM CALLBACK: USA STREAMING
            if (onChunk) {
                const stream = await selectedModel.stream(langchainMessages);
                let fullResponse = "";
                
                for await (const chunk of stream) {
                    const content = String(chunk.content || "");
                    
                    // ‚úÖ CORRE√á√ÉO: Envia apenas o NOVO conte√∫do, n√£o o acumulado
                    if (content) {
                        onChunk(content);  // ‚Üê Envia apenas este chunk
                    }
                    
                    fullResponse += content;  // ‚Üê Acumula localmente
                }

                return fullResponse;
            } 
            // ‚ùå SE N√ÉO TEM CALLBACK: USA INVOKE (sem streaming)
            else {
                const response = await selectedModel.invoke(langchainMessages);
                return String(response.content);
            }

        } catch (error) {
            console.error("Error in OpenAiChatCompletion:", error);
            throw error;
        }
    },
    {
        name: "OpenAI Chat Completion",
        run_type: "llm",
        metadata: {
            provider: "OpenAI"
        }
    }
) as (messages: any, modelType?: "advanced" | "balanced" | "fast", onChunk?: (chunk: string) => void) => Promise<string>;

// ===== toolSelector (MANT√âM ORIGINAL) =====
export const toolSelector = traceable(
    async function toolSelector(
        userInput: any, 
        sessionHistory: any[] = []
    ): Promise<{ tool: string; input: any }> {

            const systemPrompt = `
            Voc√™ √© um roteador de tarefas inteligente. Analise a INTEN√á√ÉO da pergunta, n√£o apenas as palavras exatas.

    ### Ferramentas Dispon√≠veis

    1. **specific_query_tool**: Busca informa√ß√µes espec√≠ficas sobre UMA entidade.
       
       **Quando usar:**
       - Pergunta menciona nome espec√≠fico de empresa, representante, organiza√ß√£o, etc.
       - Usu√°rio quer saber sobre UM item espec√≠fico
       - Funciona para QUALQUER propriedade: documento, plano, tipo, revenue, etc.
       
       **Exemplos (n√£o limitado a estes):**
       - "Qual o documento da iFood?"
       - "Me fale o plano da SEM PARAR"
       - "Qual o tipo da CREDIFY?"
       - "Qual a organiza√ß√£o do SEGUNDO CART√ìRIO?"
       - "Qual o revenue da iFood?"
       
       **Varia√ß√µes aceitas:**
       - "iFood tem qual documento?" ‚Üí ‚úÖ Mesma inten√ß√£o
       - "Me mostre o plano da SEM PARAR" ‚Üí ‚úÖ Mesma inten√ß√£o
       
       **Retorno:**
       {"tool": "specific_query_tool", "input": {"query": "...", "filters": {}}}

    2. **aggregate_tool**: Agrega dados por QUALQUER campo.
       
       **Quando usar:**
       - Pergunta pede compara√ß√£o, ranking ou total de M√öLTIPLOS itens
       - Usu√°rio quer ver dados agrupados
       - Funciona para: representative, company, organization, revenue, plan, company_type, etc.
       
       **Exemplos (n√£o limitado a estes):**
       - "Desempenho por representante" ‚Üí groupBy: "representative"
       - "Desempenho por empresa" ‚Üí groupBy: "company"
       - "Agregue por organiza√ß√£o" ‚Üí groupBy: "organization"
       - "Total por revenue" ‚Üí groupBy: "revenue"
       - "Agrupe por plano" ‚Üí groupBy: "plan"
       - "Empresas por tipo" ‚Üí groupBy: "company_type"
       
       **Varia√ß√µes aceitas:**
       - "Mostre cada representante" ‚Üí ‚úÖ groupBy: "representative"
       - "Ranking de vendedores" ‚Üí ‚úÖ groupBy: "representative"
       - "Compare as empresas" ‚Üí ‚úÖ groupBy: "company"
       
       **Retorno:**
       {"tool": "aggregate_tool", "input": {"query": "...", "filters": {}, "groupBy": "representative"}}
       
       **‚ö†Ô∏è IMPORTANTE:** Voc√™ DEVE especificar o campo "groupBy" no input!

    3. **hybrid_search_tool**: Busca h√≠brida (fallback).
       
       **Quando usar:**
       - Pergunta √© amb√≠gua, complexa ou n√£o se encaixa claramente nas outras tools
       - Voc√™ n√£o tem certeza qual tool usar
       
       **Exemplos:**
       - "Me explique como funciona..."
       - "An√°lise detalhada de..."
       - "Me fale sobre esses dados"
       
       **Retorno:**
       {"tool": "hybrid_search_tool", "input": {"query": "...", "filters": {}}}

    4. **none**: APENAS para cumprimentos e agradecimentos.
       
       **Quando usar:**
       - "oi", "ol√°", "bom dia"
       - "obrigado", "valeu", "at√© logo"
       
       **Retorno:**
       {"tool": "none", "input": "..."}

    ### ‚ö†Ô∏è REGRAS CR√çTICAS:
    1. **Analise a INTEN√á√ÉO**, n√£o as palavras exatas
    2. **Os exemplos s√£o ilustrativos**, n√£o limitantes
    3. **Para agrega√ß√£o, SEMPRE especifique "groupBy"**
    4. **SE VOC√ä N√ÉO TEM CERTEZA** qual tool usar, escolha **hybrid_search_tool**
    5. **Formato JSON:** Retorne APENAS o JSON da ferramenta

    ### Exemplos Completos

    **Exemplo 1: Espec√≠fica**
    Hist√≥rico: []
    Usu√°rio: "Qual o documento da iFood?"
    Retorno:
    {"tool": "specific_query_tool", "input": {"query": "documento da iFood", "filters": {}}}

    **Exemplo 2: Agrega√ß√£o por Representante**
    Hist√≥rico: []
    Usu√°rio: "desempenho por representante"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "desempenho de todos os representantes", "filters": {}, "groupBy": "representative"}}

    **Exemplo 3: Agrega√ß√£o por Plano**
    Hist√≥rico: []
    Usu√°rio: "agrupe por plano"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "dados agrupados por plano", "filters": {}, "groupBy": "plan"}}

    **Exemplo 4: Agrega√ß√£o por Tipo de Empresa**
    Hist√≥rico: []
    Usu√°rio: "compare empresas master e unique"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "compara√ß√£o entre tipos de empresa", "filters": {}, "groupBy": "company_type"}}

    **Exemplo 5: Amb√≠gua (Fallback)**
    Hist√≥rico: []
    Usu√°rio: "me explique como funciona"
    Retorno:
    {"tool": "hybrid_search_tool", "input": {"query": "explica√ß√£o sobre funcionamento", "filters": {}}}

    **Exemplo 6: Cumprimento**
    Hist√≥rico: [ ... ]
    Usu√°rio: "muito obrigado"
    Retorno:
    {"tool": "none", "input": "muito obrigado"}

    `.trim();

        const messages = [
            { role: "system", content: systemPrompt },
            ...sessionHistory,
            { role: "user", content: userInput }
        ]

        try {
            // ‚úÖ USA GPT-4o (BALANCEADO) - SEM STREAMING (mais r√°pido)
            const response = await OpenAiChatCompleiton(messages, "balanced");
            let toolCall;

            try {
                toolCall = JSON.parse(response);
            } catch (parseError) {
                console.warn("‚ö†Ô∏è Erro ao parsear JSON. Usando fallback...");
                return { 
                    tool: "hybrid_search_tool", 
                    input: { query: userInput, filters: {} } 
                };
            }

            const validTools = [
                "specific_query_tool",
                "aggregate_tool",
                "hybrid_search_tool",
                "none"
            ];
            
            if (!toolCall || !toolCall.tool || !validTools.includes(toolCall.tool)) {
                console.warn("‚ö†Ô∏è Tool inv√°lida. Usando fallback.");
                return { 
                    tool: "hybrid_search_tool", 
                    input: { query: userInput, filters: {} } 
                };
            }

            if (!toolCall.input) {
                toolCall.input = { query: userInput, filters: {} };
            }

            if (toolCall.tool === "aggregate_tool" && !toolCall.input.groupBy) {
                toolCall.input.groupBy = "company";
            }

            console.log("‚úÖ Tool selecionada:", toolCall.tool);
            return {
                tool: toolCall.tool,
                input: toolCall.input
            };

        } catch (error) {
            console.error("‚ùå Erro no toolSelector:", error);
            return { 
                tool: "hybrid_search_tool", 
                input: { query: userInput, filters: {} } 
            };
        }

    },
    {
        name: "Tool Selector",
        run_type: "chain",
        metadata: {
            purpose: "Route user query to appropriate tool",
            model: "gpt-4o"
        }
    }
) as (userInput: any, sessionHistory?: any[]) => Promise<{ tool: string; input: any }>;

// ===== getLlmResponse COM STREAMING =====
const getLlmResponse = traceable(
    async function getLlmResponse(
        messages: any, 
        systemMessageContent: any,
        modelType: "advanced" | "balanced" | "fast" = "advanced",
        onChunk?: (chunk: string) => void  // ‚Üê Callback para streaming (opcional)
    ): Promise<string> {
        const fullMessages = [
            { role: "system", content: systemMessageContent },
            ...messages
        ];

        // ‚úÖ Passa o callback para OpenAiChatCompleiton
        const response = await OpenAiChatCompleiton(fullMessages, modelType, onChunk);
        return response;
    },
    {
        name: "Get LLM Response",
        run_type: "chain",
        metadata: {
            purpose: "Generate final response with system prompt"
        }
    }
) as (messages: any, systemMessageContent: any, modelType?: "advanced" | "balanced" | "fast", onChunk?: (chunk: string) => void) => Promise<string>;

// ===== generateResponse COM STREAMING =====
export const generateResponse = traceable(
    async function generateResponse(
        sessionId: any, 
        userInput: any,
        onChunk?: (chunk: string) => void  // ‚Üê Callback para streaming (opcional)
    ): Promise<string> {
        
        await storeChatMessage(sessionId, "user", userInput);
        const sessionHistory: any[] = await retrieverSessionHistory(sessionId);
        const llmInput = [...sessionHistory];
        const { tool, input: toolInput } = await toolSelector(userInput, sessionHistory);
        console.log("üîß Tool selecionada:", tool);

        let response;

        // ===== TOOL 1: specific_query_tool (USA GPT-5) =====
        if (tool === "specific_query_tool") {
            console.log("üîç Executando specific_query_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters 
            };
            
            const contextResults = await specificQueryTool(finalToolInput);
            const context = contextResults
                .map((doc:any) => doc.document?.pageContent || JSON.stringify(doc))
                .join('\n---\n');
            
            const systemMessageContent = `
                Voc√™ √© um analista financeiro. Responda usando o contexto.
                
                REGRA CR√çTICA: Campos "InCents" t√™m 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com t√≠tulos, negrito, tabelas.
                
                Contexto:
                ${context}`.trim();
            
            // ‚úÖ USA GPT-5 (ALTA PRECIS√ÉO) + STREAMING
            response = await getLlmResponse(llmInput, systemMessageContent, "advanced", onChunk);
        }
        
        // ===== TOOL 2: aggregate_tool (USA GPT-4o) =====
        else if (tool === "aggregate_tool") {
            console.log("üìä Executando aggregate_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters,
                groupBy: toolInput.groupBy || "company"
            };
            
            const contextResults = await aggregateTool(finalToolInput);
            const contextData = JSON.parse(contextResults[0].document.pageContent);
            const context = JSON.stringify(contextData, null, 2);
            
            const systemMessageContent = `
                Voc√™ √© um analista financeiro. Os dados J√Å EST√ÉO AGREGADOS.
                N√ÉO precisa somar novamente! Apenas formate.
                
                REGRA CR√çTICA: Campos "InCents" t√™m 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com tabela. Ordene por receita l√≠quida.
                
                Contexto (j√° agregado):
                ${context}`.trim();
            
            // ‚úÖ USA GPT-4o (BALANCEADO - formata√ß√£o simples) + STREAMING
            response = await getLlmResponse(llmInput, systemMessageContent, "balanced", onChunk);
        }
        
        // ===== TOOL 3: hybrid_search_tool (USA GPT-5) =====
        else if (tool === "hybrid_search_tool") {
            console.log("üîÄ Executando hybrid_search_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters 
            };
            
            const contextResults = await hybridSearchTool(finalToolInput);
            const context = contextResults
                .map((doc:any) => doc.document?.pageContent || JSON.stringify(doc))
                .join('\n---\n');
            
            const systemMessageContent = `
                Voc√™ √© um analista financeiro. Fa√ßa uma an√°lise abrangente.
                
                REGRA CR√çTICA: Campos "InCents" t√™m 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com t√≠tulos, tabelas, listas.
                
                Contexto:
                ${context}`.trim();
            
            // ‚úÖ USA GPT-5 (ALTA INTELIG√äNCIA) + STREAMING
            response = await getLlmResponse(llmInput, systemMessageContent, "advanced", onChunk);
        }
        
        // ===== TOOL 4: calculator_tool =====
        else if (tool === "calculator_tool") {
            console.log("üßÆ Executando calculator_tool...");
            response = calculatorTool(toolInput);
            
            // Envia resposta de uma vez (n√£o √© LLM)
            if (onChunk) {
                onChunk(response);
            }
        }
        
        // ===== TOOL 5: none (USA GPT-4o-mini) =====
        else {
            console.log("üí¨ Nenhuma tool necess√°ria (cumprimento)");
            
            const systemMessageContent = `
                Voc√™ √© um assistente prestativo. Seja cordial.
                Use Markdown se necess√°rio.
            `.trim();
            
            // ‚úÖ USA GPT-4o-mini (R√ÅPIDO) + STREAMING
            response = await getLlmResponse(llmInput, systemMessageContent, "fast", onChunk);
        }

        await storeChatMessage(sessionId, "system", response);
        return response;

    },
    {
        name: "Generate Response",
        run_type: "chain",
        metadata: {
            purpose: "Main orchestration with optimized model selection and streaming"
        }
    }
) as (sessionId: any, userInput: any, onChunk?: (chunk: string) => void) => Promise<string>;
