import { traceable } from "langsmith/traceable";
import { SystemMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { advancedModel, balancedModel, fastModel } from "./config";
import { storeChatMessage, retrieverSessionHistory } from "./memory";
import { specificQueryTool, aggregateTool, hybridSearchTool, calculatorTool } from "./tools";



export const OpenAiChatCompleiton = traceable(
    async function OpenAiChatCompleiton(
        messages: any,
        modelType: "advanced" | "balanced" | "fast" = "advanced",
        onChunk?: (chunk: string) => void  
    ): Promise<string> {
        try {
            const langchainMessages = messages.map((msg: any) => {
                if (msg.role === "system") return new SystemMessage(msg.content);
                if (msg.role === "user") return new HumanMessage(msg.content);
                if (msg.role === "assistant") return new AIMessage(msg.content);
                return new HumanMessage(msg.content); 
            });

            let selectedModel;
            if (modelType === "advanced") {
                selectedModel = advancedModel;
            } else if (modelType === "balanced") {
                selectedModel = balancedModel;
            } else {
                selectedModel = fastModel;
            }

            if (onChunk) {
                const stream = await selectedModel.stream(langchainMessages);
                let fullResponse = "";
                
                for await (const chunk of stream) {
                    const content = String(chunk.content || "");
                    
                    if (content) {
                        onChunk(content);
                    }
                    
                    fullResponse += content;  
                }

                return fullResponse;
            } 
            else {
                const response = await selectedModel.invoke(langchainMessages);
                return String(response.content);
            }

        } catch (error) {
            console.error("Error in OpenAiChatCompletion:", error);
            throw error;
        }
    },
    {
        name: "OpenAI Chat Completion",
        run_type: "llm",
        metadata: {
            provider: "OpenAI"
        }
    }
) as (messages: any, modelType?: "advanced" | "balanced" | "fast", onChunk?: (chunk: string) => void) => Promise<string>;

export const toolSelector = traceable(
    async function toolSelector(
        userInput: any, 
        sessionHistory: any[] = []
    ): Promise<{ tool: string; input: any }> {

            const systemPrompt = `
            VocÃª Ã© um roteador de tarefas inteligente. Analise a INTENÃ‡ÃƒO da pergunta, nÃ£o apenas as palavras exatas.

    ### Ferramentas DisponÃ­veis

    1. **specific_query_tool**: Busca informaÃ§Ãµes especÃ­ficas sobre UMA entidade.
       
       **Quando usar:**
       - Pergunta menciona nome especÃ­fico de empresa, representante, organizaÃ§Ã£o, etc.
       - UsuÃ¡rio quer saber sobre UM item especÃ­fico
       - Funciona para QUALQUER propriedade: documento, plano, tipo, revenue, etc.
       
       **Exemplos (nÃ£o limitado a estes):**
       - "Qual o documento da iFood?"
       - "Me fale o plano da SEM PARAR"
       - "Qual o tipo da CREDIFY?"
       - "Qual a organizaÃ§Ã£o do SEGUNDO CARTÃ“RIO?"
       - "Qual o revenue da iFood?"
       
       **VariaÃ§Ãµes aceitas:**
       - "iFood tem qual documento?" â†’ âœ… Mesma intenÃ§Ã£o
       - "Me mostre o plano da SEM PARAR" â†’ âœ… Mesma intenÃ§Ã£o
       
       **Retorno:**
       {"tool": "specific_query_tool", "input": {"query": "...", "filters": {}}}

    2. **aggregate_tool**: Agrega dados por QUALQUER campo.
       
       **Quando usar:**
       - Pergunta pede comparaÃ§Ã£o, ranking ou total de MÃšLTIPLOS itens
       - UsuÃ¡rio quer ver dados agrupados
       - Funciona para: representative, company, organization, revenue, plan, company_type, etc.
       
       **Exemplos (nÃ£o limitado a estes):**
       - "Desempenho por representante" â†’ groupBy: "representative"
       - "Desempenho por empresa" â†’ groupBy: "company"
       - "Agregue por organizaÃ§Ã£o" â†’ groupBy: "organization"
       - "Total por revenue" â†’ groupBy: "revenue"
       - "Agrupe por plano" â†’ groupBy: "plan"
       - "Empresas por tipo" â†’ groupBy: "company_type"
       
       **VariaÃ§Ãµes aceitas:**
       - "Mostre cada representante" â†’ âœ… groupBy: "representative"
       - "Ranking de vendedores" â†’ âœ… groupBy: "representative"
       - "Compare as empresas" â†’ âœ… groupBy: "company"
       
       **Retorno:**
       {"tool": "aggregate_tool", "input": {"query": "...", "filters": {}, "groupBy": "representative"}}
       
       **âš ï¸ IMPORTANTE:** VocÃª DEVE especificar o campo "groupBy" no input!

    3. **hybrid_search_tool**: Busca hÃ­brida (fallback).
       
       **Quando usar:**
       - Pergunta Ã© ambÃ­gua, complexa ou nÃ£o se encaixa claramente nas outras tools
       - VocÃª nÃ£o tem certeza qual tool usar
       
       **Exemplos:**
       - "Me explique como funciona..."
       - "AnÃ¡lise detalhada de..."
       - "Me fale sobre esses dados"
       
       **Retorno:**
       {"tool": "hybrid_search_tool", "input": {"query": "...", "filters": {}}}

    4. **none**: APENAS para cumprimentos e agradecimentos.
       
       **Quando usar:**
       - "oi", "olÃ¡", "bom dia"
       - "obrigado", "valeu", "atÃ© logo"
       
       **Retorno:**
       {"tool": "none", "input": "..."}

    ### âš ï¸ REGRAS CRÃTICAS:
    1. **Analise a INTENÃ‡ÃƒO**, nÃ£o as palavras exatas
    2. **Os exemplos sÃ£o ilustrativos**, nÃ£o limitantes
    3. **Para agregaÃ§Ã£o, SEMPRE especifique "groupBy"**
    4. **SE VOCÃŠ NÃƒO TEM CERTEZA** qual tool usar, escolha **hybrid_search_tool**
    5. **Formato JSON:** Retorne APENAS o JSON da ferramenta

    ### Exemplos Completos

    **Exemplo 1: EspecÃ­fica**
    HistÃ³rico: []
    UsuÃ¡rio: "Qual o documento da iFood?"
    Retorno:
    {"tool": "specific_query_tool", "input": {"query": "documento da iFood", "filters": {}}}

    **Exemplo 2: AgregaÃ§Ã£o por Representante**
    HistÃ³rico: []
    UsuÃ¡rio: "desempenho por representante"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "desempenho de todos os representantes", "filters": {}, "groupBy": "representative"}}

    **Exemplo 3: AgregaÃ§Ã£o por Plano**
    HistÃ³rico: []
    UsuÃ¡rio: "agrupe por plano"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "dados agrupados por plano", "filters": {}, "groupBy": "plan"}}

    **Exemplo 4: AgregaÃ§Ã£o por Tipo de Empresa**
    HistÃ³rico: []
    UsuÃ¡rio: "compare empresas master e unique"
    Retorno:
    {"tool": "aggregate_tool", "input": {"query": "comparaÃ§Ã£o entre tipos de empresa", "filters": {}, "groupBy": "company_type"}}

    **Exemplo 5: AmbÃ­gua (Fallback)**
    HistÃ³rico: []
    UsuÃ¡rio: "me explique como funciona"
    Retorno:
    {"tool": "hybrid_search_tool", "input": {"query": "explicaÃ§Ã£o sobre funcionamento", "filters": {}}}

    **Exemplo 6: Cumprimento**
    HistÃ³rico: [ ... ]
    UsuÃ¡rio: "muito obrigado"
    Retorno:
    {"tool": "none", "input": "muito obrigado"}

    `.trim();

        const messages = [
            { role: "system", content: systemPrompt },
            ...sessionHistory,
            { role: "user", content: userInput }
        ]

        try {
            const response = await OpenAiChatCompleiton(messages, "balanced");
            let toolCall;

            try {
                toolCall = JSON.parse(response);
            } catch (parseError) {
                console.warn("âš ï¸ Erro ao parsear JSON. Usando fallback...");
                return { 
                    tool: "hybrid_search_tool", 
                    input: { query: userInput, filters: {} } 
                };
            }

            const validTools = [
                "specific_query_tool",
                "aggregate_tool",
                "hybrid_search_tool",
                "none"
            ];
            
            if (!toolCall || !toolCall.tool || !validTools.includes(toolCall.tool)) {
                console.warn("âš ï¸ Tool invÃ¡lida. Usando fallback.");
                return { 
                    tool: "hybrid_search_tool", 
                    input: { query: userInput, filters: {} } 
                };
            }

            if (!toolCall.input) {
                toolCall.input = { query: userInput, filters: {} };
            }

            if (toolCall.tool === "aggregate_tool" && !toolCall.input.groupBy) {
                toolCall.input.groupBy = "company";
            }

            console.log("âœ… Tool selecionada:", toolCall.tool);
            return {
                tool: toolCall.tool,
                input: toolCall.input
            };

        } catch (error) {
            console.error("âŒ Erro no toolSelector:", error);
            return { 
                tool: "hybrid_search_tool", 
                input: { query: userInput, filters: {} } 
            };
        }

    },
    {
        name: "Tool Selector",
        run_type: "chain",
        metadata: {
            purpose: "Route user query to appropriate tool",
            model: "gpt-4o"
        }
    }
) as (userInput: any, sessionHistory?: any[]) => Promise<{ tool: string; input: any }>;

const getLlmResponse = traceable(
    async function getLlmResponse(
        messages: any, 
        systemMessageContent: any,
        modelType: "advanced" | "balanced" | "fast" = "advanced",
        onChunk?: (chunk: string) => void  
    ): Promise<string> {
        const fullMessages = [
            { role: "system", content: systemMessageContent },
            ...messages
        ];

        
        const response = await OpenAiChatCompleiton(fullMessages, modelType, onChunk);
        return response;
    },
    {
        name: "Get LLM Response",
        run_type: "chain",
        metadata: {
            purpose: "Generate final response with system prompt"
        }
    }
) as (messages: any, systemMessageContent: any, modelType?: "advanced" | "balanced" | "fast", onChunk?: (chunk: string) => void) => Promise<string>;


export const generateResponse = traceable(
    async function generateResponse(
        sessionId: any, 
        userInput: any,
        onChunk?: (chunk: string) => void  
    ): Promise<string> {
        
        await storeChatMessage(sessionId, "user", userInput);
        const sessionHistory: any[] = await retrieverSessionHistory(sessionId);
        const llmInput = [...sessionHistory];
        const { tool, input: toolInput } = await toolSelector(userInput, sessionHistory);
        console.log("ðŸ”§ Tool selecionada:", tool);

        let response;

        
        if (tool === "specific_query_tool") {
            console.log("ðŸ” Executando specific_query_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters 
            };
            
            const contextResults = await specificQueryTool(finalToolInput);
            const context = contextResults
                .map((doc:any) => doc.document?.pageContent || JSON.stringify(doc))
                .join('\n---\n');
            
            const systemMessageContent = `
                VocÃª Ã© um analista financeiro. Responda usando o contexto.
                
                REGRA CRÃTICA: Campos "InCents" tÃªm 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com tÃ­tulos, negrito, tabelas.
                
                Contexto:
                ${context}`.trim();
            
            
            response = await getLlmResponse(llmInput, systemMessageContent, "advanced", onChunk);
        }
        
        
        else if (tool === "aggregate_tool") {
            console.log("ðŸ“Š Executando aggregate_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters,
                groupBy: toolInput.groupBy || "company"
            };
            
            const contextResults = await aggregateTool(finalToolInput);
            const contextData = JSON.parse(contextResults[0].document.pageContent);
            const context = JSON.stringify(contextData, null, 2);
            
            const systemMessageContent = `
                VocÃª Ã© um analista financeiro. Os dados JÃ ESTÃƒO AGREGADOS.
                NÃƒO precisa somar novamente! Apenas formate.
                
                REGRA CRÃTICA: Campos "InCents" tÃªm 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com tabela. Ordene por receita lÃ­quida.
                
                Contexto (jÃ¡ agregado):
                ${context}`.trim();
            
            
            response = await getLlmResponse(llmInput, systemMessageContent, "balanced", onChunk);
        }
        
        
        else if (tool === "hybrid_search_tool") {
            console.log("ðŸ”€ Executando hybrid_search_tool...");
            
            const finalFilters = { "month": sessionId };
            const finalToolInput = { 
                query: toolInput.query || userInput, 
                filters: finalFilters 
            };
            
            const contextResults = await hybridSearchTool(finalToolInput);
            const context = contextResults
                .map((doc:any) => doc.document?.pageContent || JSON.stringify(doc))
                .join('\n---\n');
            
            const systemMessageContent = `
                VocÃª Ã© um analista financeiro. FaÃ§a uma anÃ¡lise abrangente.
                
                REGRA CRÃTICA: Campos "InCents" tÃªm 4 casas decimais.
                SEMPRE DIVIDA POR 10.000 para converter para Reais.
                
                Use Markdown com tÃ­tulos, tabelas, listas.
                
                Contexto:
                ${context}`.trim();
            
            
            response = await getLlmResponse(llmInput, systemMessageContent, "advanced", onChunk);
        }
        
        
        else if (tool === "calculator_tool") {
            console.log("ðŸ§® Executando calculator_tool...");
            response = calculatorTool(toolInput);
            
            
            if (onChunk) {
                onChunk(response);
            }
        }
        
        
        else {
            console.log("ðŸ’¬ Nenhuma tool necessÃ¡ria (cumprimento)");
            
            const systemMessageContent = `
                VocÃª Ã© um assistente prestativo. Seja cordial.
                Use Markdown se necessÃ¡rio.
            `.trim();
            
            
            response = await getLlmResponse(llmInput, systemMessageContent, "fast", onChunk);
        }

        await storeChatMessage(sessionId, "system", response);
        return response;

    },
    {
        name: "Generate Response",
        run_type: "chain",
        metadata: {
            purpose: "Main orchestration with optimized model selection and streaming"
        }
    }
) as (sessionId: any, userInput: any, onChunk?: (chunk: string) => void) => Promise<string>;
